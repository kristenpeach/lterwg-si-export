---
title: "arikaree_NWT_watershed"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

###Step 1: Load packages

```{r}

library(metajam)  
library(udunits2)
# For wrangling the data
library(readr)
library(tidyr)
library(dplyr)
library(purrr)
library(stringr)
library(metajam)
library(tidyverse) # for convenience
library(here) # for file path management
library(stringdist) # for first pass of naming-matching
library(vctrs) # for joining tables
library(readxl) # for reading the template excel sheet
library(naniar)
library(openxlsx)

```


####Step 2: Find the link to the dataset
Go to the web address for the dataset and find the download button for the data. 

In our case the link is: https://portal.edirepository.org/nis/dataviewer?packageid=knb-lter-nwt.104.11&entityid=1f6d60d3f8fb0b902358b3b6da579f67

#### Step 3: Choose where you want the files to be saved
In our case, we'll just put it into the metajam_example folder.

```{r}


#eg desired_path_to_data <- "~/Desktop"
desired_path_to_data <- "~/SI_river_data"

# create the folder if it does not exist yet
dir.create(desired_path_to_data, showWarnings = FALSE)


```

#### Step 4: Download the data by pasting the link you just copied

```{r}

my_data_url <- "https://portal.edirepository.org/nis/dataviewer?packageid=knb-lter-nwt.104.11&entityid=1f6d60d3f8fb0b902358b3b6da579f67"

# this will download the data into a folder and save the path to that folder
downloaded_data <- download_d1_data(data_url = my_data_url, path = desired_path_to_data)


```


#### Step 5: Now read in the data (with all the metadata)

```{r}


my_data <- read_d1_files(downloaded_data, na = c("u", "NP", "DNS", "EQCL"))

arikaree_NWT_data <- my_data$data



```

Data cleanup

```{r}

#these columns already have the info we want but we need to change the column names to match the template
arikaree_NWT_data <- arikaree_NWT_data %>% 
  rename('Sampling Date' = 'date')

arikaree_NWT_data <- arikaree_NWT_data %>% 
  rename('Site/Stream Name' = 'local_site')

arikaree_NWT_data <- arikaree_NWT_data %>% 
  rename('LTER' = 'LTER_site')

arikaree_NWT_data <- arikaree_NWT_data %>% 
  rename('Time' = 'time')

arikaree_NWT_data <- arikaree_NWT_data %>% 
  rename('Conductivity' = 'cond')

arikaree_NWT_data <- arikaree_NWT_data %>% 
  rename('alkalinity' = 'alkal')

names(arikaree_NWT_data) <- gsub("[^0-9A-Za-z///' ]"," ", names(arikaree_NWT_data))

```

Read in template
```{r}

template <- read_excel(here("metajam_example", "Stream_Data_Template.xlsx"), 
                               sheet = "Raw Data",
                               col_types = "text"
                               ) %>%
  mutate(`Sampling Date` = as.Date(`Sampling Date`))

#Adding a column for NO3 so that the template includes both an NO3 column and NOx column
template <- template %>% 
  add_column('NOx' = '.', .before = "NO3")


```

Fuzzy Match

```{r}
# Start by matching by closest name as a first pass. 
  # Note that we match the lower case names
  # Note that the weight i= 0.1 says that we will be more likely to match if watershed 1 = template + extra
(fuzzy_match <- tibble(template = names(template)) %>%
   mutate(watershed11 = names(arikaree_NWT_data)[amatch(tolower(template), tolower(names(arikaree_NWT_data)), maxDist = 1, weight = c(d=1,i=0.1,s=1,t=1))])
)


```

```{r}


# Fill in the columns that didn't match, and correct the wrongly corrected matches
# continue in the same way until you've filled out everything you could.
  # Note: NA_character_ is just NA but of the character type.
(lookup_table <- fuzzy_match %>%
  mutate(watershed11 = case_when(
    template %in% c("DIC","TKN", "PON", "NOx") ~ NA_character_,
    TRUE ~ watershed11)))

lookup_table <- lookup_table %>%
  filter(!is.na(watershed11))


correct_colnames <- lookup_table$template

correct_colnames <- as.character(correct_colnames)



```



```{r}



new_datatable <- arikaree_NWT_data %>% 
        select(one_of(c(correct_colnames)))


```